[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README_CH.md) | [**ğŸŒEnglish**](./README.md) |

# Recurrent Context Compression: Efficiently Expanding the Context Window of LLM
[[paper]()]


## TL;DR
è¿™é¡¹å·¥ä½œä»‹ç»äº†ä¸€ç§åä¸ºå¾ªç¯ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆRCCï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æœ‰é™çš„å­˜å‚¨ç©ºé—´å†…é«˜æ•ˆåœ°æ‰©å¤§LLMçš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ‹¥æœ‰32xä¸Šä¸‹æ–‡å‹ç¼©çš„æ¨¡å‹ï¼Œå‡†ç¡®çš„æ¥è¯´è¿™ä¸ªæ¨¡å‹å¯ä»¥å°†æ¨¡å‹çš„è¾“å…¥æ–‡æœ¬å‹ç¼©ä¸º1/32ï¼Œèƒ½å¤Ÿåœ¨é•¿åºåˆ—ä¸­å¤§å¹…å‡å°‘æ˜¾å­˜æ¶ˆè€—ã€‚

ç”±äºå¾®è°ƒæ•°æ®é›†çš„é™åˆ¶ï¼Œè¯¥æ¨¡å‹ç›®å‰ä»…æ”¯æŒQAä»»åŠ¡ã€‚



## News

- [2024/6/9] æˆ‘ä»¬å‘å¸ƒäº†RCC-Pythia-1.4bæ¨¡å‹å’Œ[è®ºæ–‡]()

## Model Overview
RCCç”±ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œç¼–ç å™¨ä¸è§£ç å™¨çš„æƒé‡éƒ½ç”±å¤§å‹è¯­è¨€æ¨¡å‹åˆå§‹åŒ–è€Œæ¥ã€‚ç»è¿‡è®­ç»ƒçš„ç¼–ç å™¨èƒ½å¤Ÿå°†å›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å‹ç¼©æˆæ›´ç´§å‡‘çš„å½¢å¼ï¼ŒæŒ‡ä»¤å’Œæ™®é€šæ–‡æœ¬éƒ½å¯ä»¥å½“ä½œä¸Šä¸‹æ–‡è¿›è¡Œå‹ç¼©ã€‚å½“ä¸Šä¸‹æ–‡ä¿¡æ¯è¶…è¿‡å›ºå®šé•¿åº¦æ—¶ï¼Œç¼–ç å™¨æ‰§è¡Œå¾ªç¯å‹ç¼©å¹¶å°†æ‰€æœ‰å‹ç¼©ç‰¹å¾å‘é‡è¿æ¥èµ·æ¥ã€‚è§£ç å™¨åˆ©ç”¨å‹ç¼©ç‰¹å¾å‘é‡ä½œä¸ºå†å²çŠ¶æ€å‘é‡çš„è¾“å…¥ï¼Œå®Œæˆæœ€ç»ˆçš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚
åŒæ—¶æˆ‘ä»¬ç ”ç©¶äº†åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æŒ‡ä»¤ä¸ä¸Šä¸‹æ–‡åŒæ—¶è¢«å‹ç¼©æ—¶æ¨¡å‹çš„å›ç­”è¾ƒå·®è¿™ä¸€é—®é¢˜ï¼Œå¹¶æå‡ºäº†æŒ‡ä»¤é‡å»ºçš„æ–¹æ³•ç¼“è§£äº†è¿™ä¸€é—®é¢˜ã€‚æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨æˆ‘ä»¬çš„è®ºæ–‡ä¸­æ‰¾åˆ°ã€‚

**The structure of the encoder and decoder in RCC layer i.**
<img src=figures/model_structure.png alt="" width="800">


**Memory Consumption of Different Models with Increasing Length. Left: Pythia-1.4b, Right: RCC model using Pythia-1.4b for both encoder and decoder. Both models utilize FlashAttention-2**

<img src=figures/memory_size.png alt="" width="800">





## Usage

### Model use Cases

```bash
import torch
from transformers import AutoTokenizer
from  model.modeling_mcp2 import  GPTNeoXForCausalLM
from  transformers import GPTNeoXForCausalLM as GPTNeoXForCausalLM2
from safetensors.torch import load_file,save_file
import os
import json
from chat_example import load_and_merge_safetensors



#RCC-pythia, Example Usage
path = "EleutherAI/pythia-1.4b"
tokenizer = AutoTokenizer.from_pretrained(path)
model = GPTNeoXForCausalLM.from_pretrained(path,_attn_implementation="flash_attention_2",torch_dtype=torch.bfloat16).cuda()
model.gpt_neox.rcc_encoder = GPTNeoXForCausalLM2.from_pretrained(path,_attn_implementation="flash_attention_2",torch_dtype=torch.bfloat16).cuda()
#The length of each input to the decoder
model.gpt_neox.rcc_encoder_length = 2048

#The compression ratio is fixed to 32
model.gpt_neox.mem_len = 32

#Weights need to be downloaded
directory_path = "ckp/RCC_Ins_Reconstruction"
merged_state_dict = load_and_merge_safetensors(directory_path)
model.load_state_dict(merged_state_dict)


input_context = "The cat sat on the sofa."+ "The dog sat on the stool."*1000
input_context = input_context+"\n\n"+"Where does the cat sit?"
compressed_id = tokenizer(input_context, return_tensors="pt")["input_ids"].cuda()
#A chat example of instruction reconstruction
prompt_fixed = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n"
#Chat example of manual instruction input
#prompt_fixed = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\nWhere is the cat sitting?"
prompt_ids = tokenizer(prompt_fixed ,return_tensors="pt")["input_ids"].cuda() 
#Note that you must add this code or you may run out of memory
model.eval()
with torch.no_grad():
    generate_id = model.generate(compressed_id=compressed_id,prompt_ids=prompt_ids,max_new_tokens=100,eos_token=0)
pred = tokenizer.decode(generate_id[0], skip_special_tokens=True)
#pred = pred.split("assistant\n")[-1]
print(pred)
"""
Where does the cat sit?<|im_end|>
<|im_start|>assistant
The cat sits on the sofa.
"""

```



### Run more cases

```bash
CUDA_VISIBLE_DEVICES=0 python test.py
```





## FAQ

1. **RCCåœ¨åŸºç¡€æ¨¡å‹ä¸Šæ‰©å±•å¤šé•¿çš„ä¸Šä¸‹æ–‡**
æœ€ç»ˆçš„ç†è®ºåºåˆ—é•¿åº¦ç”±è§£ç å™¨çš„å‹ç¼©ç‡å’Œç¼–ç å™¨çª—å£é•¿åº¦çš„ä¹˜ç§¯å†³å®šã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‹ç¼©ç‡è®¾ç½®ä¸º32ï¼Œå¹¶ä¸”ç¼–ç å™¨è®¾è®¡ä¸ºå¤„ç†2048ä¸ªtokenï¼Œé‚£ä¹ˆå¾—åˆ°çš„ç†è®ºé•¿åº¦å°†æ˜¯32ä¹˜ä»¥2048ã€‚

2. **ç›®å‰RCCçš„æ€§èƒ½å¦‚ä½•ï¼Ÿ**
ç”±äºå¾®è°ƒæ•°æ®é›†çš„é•¿åº¦çº¦æŸï¼Œå½“ä½¿ç”¨é•¿äºå¾®è°ƒæ•°æ®é›†é•¿åº¦çš„åºåˆ—è¿›è¡Œæ¨ç†æ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚


## TODOs
æˆ‘ä»¬è¿˜å°†ç»§ç»­åŸºäºæ›´å¼ºå¤§çš„å¼€æºæ¨¡å‹(åŒ…æ‹¬qwen2ç³»åˆ—ã€llama3ç³»åˆ—å’Œmistralç³»åˆ—)è®­ç»ƒå’Œå‘å¸ƒRCCæ¨¡å‹ã€‚

**è¯·ç»§ç»­å…³æ³¨æˆ‘ä»¬çš„æœ€æ–°æ¶ˆæ¯ã€‚**

- [x] Updating inference code.
- [] Updating the training code


## Motivation
æˆ‘ä»¬ç¼–ç å™¨çš„è®¾è®¡ï¼Œå—åˆ°äº†åŸºäºMambaçš„LLMå¯å‘ã€‚ Mambaæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå®ƒå’ŒRNNç±»ä¼¼ã€‚åœ¨Mambaä¸­ï¼Œå½“å‰tokenåªéœ€è·å–ä¸Šä¸€ä¸ªæ—¶åˆ»çš„çŠ¶æ€å‘é‡å³å¯å®Œæˆå½“å‰æ¨ç†æ­¥éª¤ï¼Œä½†æ˜¯å½“ä¸Šä¸‹æ–‡é•¿åº¦å˜é•¿æ—¶ï¼ŒMambaå›ç­”çš„æ•ˆæœä¼šå˜å·®ã€‚è¿™è¯´æ˜äº†Mambaä¸­ä¸€ä¸ªæ—¶åˆ»çš„çŠ¶æ€å‘é‡åªèƒ½å­˜å‚¨çš„ä¸€å®šé•¿åº¦çš„å†å²ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæŠ˜ä¸­çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼Œå¯ä»¥å°†å®ƒåˆ†å‰²ä¸ºå›ºå®šé•¿åº¦çš„çŸ­åºåˆ—ï¼Œå¹¶å¾ªç¯çš„å°†æ¯ä¸ªçŸ­åºåˆ—å‹ç¼©åˆ°ä¸€ä¸ªçŠ¶æ€å‘é‡ä¸­ã€‚æˆ‘ä»¬å°†æ¯ä¸ªçŸ­åºåˆ—çš„çŠ¶æ€å‘é‡æ‹¼æ¥èµ·æ¥ä½œä¸ºæ¨ç†æ—¶æœŸçš„å†å²çŠ¶æ€ä¿¡æ¯ï¼Œè¿™æ ·æœ€å¤§é™åº¦çš„ä¿ç•™äº†å®Œæ•´çš„å†å²ä¿¡æ¯ï¼ŒåŒæ—¶ä¹Ÿèƒ½åˆ©ç”¨æ¨¡å‹çš„å‹ç¼©èƒ½åŠ›èŠ‚çº¦å†…å­˜ã€‚ åœ¨æœ¬æ–‡ä¸­æˆ‘ä»¬ä½¿ç”¨å‹ç¼©ç‡æ¥åæ˜ ä¸€ä¸ªæ—¶åˆ»çš„çŠ¶æ€å‘é‡å­˜å‚¨çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæˆ‘ä»¬åœ¨å®éªŒä¸­å‘ç°Transformerä¹Ÿå…·å¤‡è¿™ç§èƒ½åŠ›ï¼Œè¿™æ˜¯å› ä¸ºTransformerå¯ä»¥çœ‹ä½œä¸€ä¸ªç‰¹æ®ŠçŠ¶æ€ç©ºé—´æ¨¡å‹æˆ–è€…RNNæ¨¡å‹ï¼Œæœ€è¿‘çš„ç ”ç©¶ä¹Ÿè¡¨æ˜æ³¨æ„åŠ›å¯ä»¥è¢«è§†ä¸ºRNNã€‚
è§£ç å™¨æˆ‘ä»¬é‡‡ç”¨Transformerï¼Œè¿™æ ·å°±èƒ½è®¿é—®åˆ°ä»»ä½•ä½ç½®ä¸Šçš„å‹ç¼©å‘é‡ã€‚





## Citation

å¦‚æœæ‚¨å‘ç°RCCå¯¹æ‚¨çš„é¡¹ç›®å’Œç ”ç©¶æœ‰ç”¨æˆ–ç›¸å…³ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡:

```bibtex

```